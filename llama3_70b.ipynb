{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaXUOWvOqfdWqSaIwKB9TM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lykskai/HodgkinAvatar/blob/text-model/llama3_70b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Install Groq API to use with Llama 3"
      ],
      "metadata": {
        "id": "36t5EFoQ8l0d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxGX-ZBQ8C9w",
        "outputId": "3469ab29-d971-4cac-a945-071875652cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from groq) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.11/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->groq) (2.27.2)\n",
            "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.9/121.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.18.0\n"
          ]
        }
      ],
      "source": [
        "pip install groq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1) Import necessary libraries"
      ],
      "metadata": {
        "id": "CxS5ulNG8s9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os             # provides a way to interact with OS, allows us to do I/O tasks\n",
        "from groq import Groq # interact with GroqAPI"
      ],
      "metadata": {
        "id": "sxvhSBtd8u-H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Create an instance of the Groq client"
      ],
      "metadata": {
        "id": "7b_IHLUD86Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('Groq')  # Retrieve the stored API key\n",
        "\n",
        "client = Groq(\n",
        "    api_key=api_key  # Use the retrieved key\n",
        ")\n"
      ],
      "metadata": {
        "id": "GuNc1Mrk8-Eo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Generate a Chat Completion"
      ],
      "metadata": {
        "id": "stPIwwJW9T-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    # messages: list of mesasge objects, has role and content. i.e. user role's content is a prompt\n",
        "    # model: the machine learning model used for generating the completion\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Who are you?\",\n",
        "        }\n",
        "    ],\n",
        "    model= \"llama3-70b-8192\",\n",
        ")"
      ],
      "metadata": {
        "id": "Xw_fUTc79XIk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4) Generate the printed content"
      ],
      "metadata": {
        "id": "0knIQ76k91Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat_completion.choices[0].message.content) # This code prints the content of the first choice"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhJWdzer93tp",
        "outputId": "86caa40f-c04d-4d94-bdf1-8d59ac100734"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am LLaMA, an AI assistant developed by Meta AI that can understand and respond to human input in a conversational manner. I'm not a human, but a computer program designed to simulate conversation and answer questions to the best of my knowledge. I can provide information on a wide range of topics, from science and history to entertainment and culture. I can also help with language-related tasks, such as language translation, grammar correction, and text summarization.\n",
            "\n",
            "I'm a large language model, trained on a massive dataset of text from the internet and can generate human-like responses. My primary function is to assist and provide helpful information to users, and I strive to do so in a neutral and respectful manner.\n",
            "\n",
            "I don't have personal opinions or feelings, but I'm designed to be friendly and engaging. I can understand and respond to emotions, but I don't experience them myself. I'm simply a computer program designed to provide information and assist with tasks to the best of my abilities.\n",
            "\n",
            "So, how can I help you today?\n"
          ]
        }
      ]
    }
  ]
}