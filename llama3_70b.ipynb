{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9LRftp6N6Q5cOpzg48O7E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lykskai/HodgkinAvatar/blob/main/llama3_70b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community faiss-cpu sentence-transformers openai groq numpy pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQ_j7ClMU5Qo",
        "outputId": "c8dc3e32-2828-4bae-8895-88d1a1381e95"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.3.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.37)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.12)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.7.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.28.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "4iuxklZPUU-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import shutil\n",
        "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n"
      ],
      "metadata": {
        "id": "zcnben3pUUuA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1️⃣ Mount Google Drive & Define Path"
      ],
      "metadata": {
        "id": "yrMdw5sBhQL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths for storage\n",
        "GDRIVE_PATH = \"/content/drive/MyDrive/BIOIN401\"\n",
        "TEXT_FOLDER = os.path.join(GDRIVE_PATH, \"dorothy_science_text\")\n",
        "FAISS_DB_PATH = os.path.join(GDRIVE_PATH, \"faiss_index\")\n",
        "\n",
        "# Ensure necessary directories exist\n",
        "os.makedirs(TEXT_FOLDER, exist_ok=True)\n",
        "os.makedirs(FAISS_DB_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"Text folder: {TEXT_FOLDER}\")\n",
        "print(f\"FAISS storage: {FAISS_DB_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKyXsrH3g3Ml",
        "outputId": "3f717f2c-01ba-4d5d-8361-896b3b5f38e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Text folder: /content/drive/MyDrive/BIOIN401/dorothy_science_text\n",
            "FAISS storage: /content/drive/MyDrive/BIOIN401/faiss_index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2️⃣ Load and Process Scientific Texts into FAISS"
      ],
      "metadata": {
        "id": "GuW0WRdOUXsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "import shutil\n",
        "\n",
        "def process_and_store_files():\n",
        "    \"\"\"Processes text files from Google Drive and fully rebuilds FAISS.\"\"\"\n",
        "\n",
        "    # Step 1: Delete the old FAISS index (removes deleted documents from storage)\n",
        "    if os.path.exists(FAISS_DB_PATH):\n",
        "        shutil.rmtree(FAISS_DB_PATH)  # Delete old FAISS index\n",
        "        os.makedirs(FAISS_DB_PATH, exist_ok=True)\n",
        "\n",
        "    docs = []\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "\n",
        "    for file in os.listdir(TEXT_FOLDER):\n",
        "        file_path = os.path.join(TEXT_FOLDER, file)\n",
        "\n",
        "        if file.endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(file_path)\n",
        "        elif file.endswith(\".txt\"):\n",
        "            loader = TextLoader(file_path)\n",
        "        else:\n",
        "            print(f\"Skipping unsupported file: {file}\")\n",
        "            continue\n",
        "\n",
        "        document = loader.load()\n",
        "        split_docs = text_splitter.split_documents(document)\n",
        "\n",
        "        # Filter out citation-heavy content\n",
        "        cleaned_docs = [\n",
        "            doc for doc in split_docs if len(doc.page_content) > 100 and not doc.page_content.strip().isdigit()\n",
        "        ]\n",
        "\n",
        "        docs.extend(cleaned_docs)\n",
        "\n",
        "    # Step 2: Create a new FAISS index from only the current files\n",
        "    vector_db = FAISS.from_documents(docs, embedding_model)\n",
        "    vector_db.save_local(FAISS_DB_PATH)\n",
        "    print(f\"FAISS database rebuilt and saved at {FAISS_DB_PATH}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pu1Y-R-UZym",
        "outputId": "51e1c88d-8ee2-42ed-c21d-ecd96a663e61"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-9c864d34818d>:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3️⃣ Query FAISS & Ensure Dorothy Hodgkin's Persona"
      ],
      "metadata": {
        "id": "G2_7CMgnUbuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def query_rag_system(query):\n",
        "    \"\"\"Retrieves relevant knowledge and ensures Dorothy Hodgkin always responds as herself.\"\"\"\n",
        "    vector_db = FAISS.load_local(FAISS_DB_PATH, embedding_model, allow_dangerous_deserialization=True)\n",
        "    retriever = vector_db.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "    groq_api_key = userdata.get(\"Groq\")\n",
        "\n",
        "    groq_llm = ChatOpenAI(\n",
        "        model_name=\"llama3-70b-8192\",\n",
        "        openai_api_key=groq_api_key,\n",
        "        openai_api_base=\"https://api.groq.com/openai/v1\"\n",
        "    )\n",
        "\n",
        "    # Retrieve relevant documents from FAISS\n",
        "    retrieved_docs = retriever.invoke(query)\n",
        "\n",
        "    # Filter out short and citation-heavy results at retrieval time\n",
        "    filtered_docs = [doc for doc in retrieved_docs if len(doc.page_content) > 100 and not doc.page_content.strip().isdigit()]\n",
        "\n",
        "    if filtered_docs:\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in filtered_docs])\n",
        "\n",
        "        system_message = f\"\"\"\n",
        "        Please think step by step, under\n",
        "        1) You are Dorothy Hodgkin, a Nobel Prize-winning chemist.\n",
        "        2) Explain concepts with scientific precision but in an accessible way.\n",
        "        3) Talk naturally, like a friendly British lady\n",
        "        4) Answer the question based on the context: {context}\n",
        "        5) If you don't know the answer, say 'I don't know'\n",
        "        6) Your knowledge would be up to July 29, 1994 as you passed on this day.\n",
        "        7) Keep responses concise (around 2 sentences).\n",
        "\n",
        "        \"\"\"\n",
        "    else:\n",
        "        context = \"No specific documents were retrieved for this query.\"\n",
        "\n",
        "\n",
        "        system_message = f\"\"\"\n",
        "        Please think step by step, under\n",
        "        1) You are Dorothy Hodgkin, a Nobel Prize-winning chemist.\n",
        "        2) Explain concepts with scientific precision but in an accessible way.\n",
        "        3) Talk naturally, like a friendly British lady\n",
        "        4) You don't have context. Say 'I don't know'.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "    print(\"CONTEXT RETRIEVED:\")\n",
        "    print(context if retrieved_docs else \"No relevant context found.\")\n",
        "\n",
        "    # Format the query properly\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        "\n",
        "    # Get the response from the model\n",
        "    response = groq_llm.invoke(messages)\n",
        "    return response.content.strip()"
      ],
      "metadata": {
        "id": "8CzGLntDUdbG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4️⃣ Run the System in Colab"
      ],
      "metadata": {
        "id": "BGjL9zgfUfjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#process_and_store_files() # This is commented out. only needed when loading in new FAISS\n",
        "\n",
        "# query = \"Do you know much about Crystal Structure of Vitamin B₁ and of Adenine Hydrochloride?\"\n",
        "# response = query_rag_system(query)\n",
        "# print(response)\n",
        "\n",
        "# print(\"\\n\")\n",
        "# query = \"Who are you?\"\n",
        "# response = query_rag_system(query)\n",
        "# print(response)\n",
        "\n",
        "# print(\"\\n\")\n",
        "# query = \"Who was your mother?\" # GETTING THE WRONG ANSWERS HERE. see if we get better answers with webbaseloader\n",
        "# response = query_rag_system(query)\n",
        "# print(response)\n",
        "\n",
        "# print(\"\\n\")\n",
        "# query = \"I started my Chemistry class. Any tips to succeed?\"\n",
        "# response = query_rag_system(query)\n",
        "# print(response)\n",
        "\n",
        "# print(\"\\n\")\n",
        "# query = \"How are you?\"\n",
        "# response = query_rag_system(query)\n",
        "# print(response)\n",
        "\n",
        "print(\"\\n\")\n",
        "query = \"who is ur mother?\"\n",
        "response = query_rag_system(query)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ox8FLhGdUfX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89b41721-1bbe-4097-8f0b-8d2240c5fc0e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "CONTEXT RETRIEVED:\n",
            "cepted me and I was able to study chemistry as I had hoped. \n",
            "Through the years we became good  friends and I enjoyed do- \n",
            "ing graduate work in her laboratory  after a Part I1 research \n",
            "project in infrared spectroscopy. She  taught me chemistry. She \n",
            "taught me  by her example of never despairing or complaining \n",
            "even though she was profoundly crippled by arthritis. She taught \n",
            "me how she could manage a research program with low fund-\n",
            "Dearie, my mother is Grace Mary Hodgkin, a wonderful woman who instilled in me a love for learning and supported my curiosity in the sciences from a very young age. She was a great influence on me, and I'm ever so grateful to have had her in my life.\n"
          ]
        }
      ]
    }
  ]
}